{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You want to Design a Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Keras' Sequential Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Libraries\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start Neural Network\n",
    "network = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add fully Connected Layer with a ReLU activation function\n",
    "#  Most widely used Activation function in the world right now.\n",
    "# Condition:\n",
    "#       F(z) is zero when z<0 , F(z) is equal to z when z>= 0\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(10,)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Fully connected layer with a ReLU activation function.\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add fully connected layer with a sigmoid activation function \n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile Neural Network\n",
    "network.compile(loss=\"binary_crossentropy\", # Cross-entropy \n",
    "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"]) # Accuracy performance metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Complete Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Libraries\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# Start Neural Network\n",
    "network = models.Sequential()\n",
    "\n",
    "# Add fully Connected Layer with a ReLU activation function\n",
    "#  Most widely used Activation function in the world right now.\n",
    "# Condition:\n",
    "#       F(z) is zero when z<0 , F(z) is equal to z when z>= 0\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(10,)))\n",
    "\n",
    "# Add Fully connected layer with a ReLU activation function.\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "# Add fully connected layer with a sigmoid activation function \n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "# Compile Neural Network\n",
    "network.compile(loss=\"binary_crossentropy\", # Cross-entropy \n",
    "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"]) # Accuracy performance metric\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To construct a feedforward neural network in Keras, we need to make a number of choices about both the network architecture and training process. \n",
    "Remember that each unit in the hidden layers: \n",
    "1. Receives a number of inputs. \n",
    "2. Weights each input by a parameter value. \n",
    "3. Sums together all weighted inputs along with some bias (typically 1).\n",
    "4. Most often then applies some function (called an activation function).\n",
    "5. Sends the output on to units in the next layer. \n",
    "\n",
    "First, for each layer in the hidden and output layers we must define the number of units to include in the layer and the activation function. \n",
    "Overall, the more units we have in a layer, the more our network is able to learn complex patterns.\n",
    "However, more units might make our network overfit the training data in a way detrimental to the performance on the test data. \n",
    "\n",
    "For hidden layers, a popular activation function is the rectified linear unit (ReLU):\n",
    "f (z) = max(0,z)\n",
    "where z is the sum of the weighted inputs and bias. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
